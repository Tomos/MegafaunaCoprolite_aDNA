# NOTE: Changes to the config file in the middle of the pipeline
# may force previous steps to be rerun.
#  
# ===============================================================
#
# READPREP: QC reads, generate and deduplicate query kmers.
#
# ===============================================================

# The fastq files for processing are found by matching the
# the fastq_pattern provided below (in Snakemake wildcard
# format).
# The default is "reads/{sample}_R{pair}_001.fastq"
# and should work for illumina formatted file names.
# The "sample" keyword is REQUIRED and will be used to name
# all output files. The "pair" keyword can be used to combine
# paired end reads, they will be concatenated after QC and
# analyzed together. Other keywords and regex can be added
# to make sample name more specific.
# See Snakemake documentation (https://snakemake.readthedocs.io)
# for wildcard constraints.
# ---------
# Examples
# ---------
# For file myreads/HUMAN-Blank_S3_R1_001.fq
# the pattern "myreads/HUMAN-{sample}_R{pair}.fq" will
# name this sample "Blank_S3". If an R2 file with the
# same pattern exists in the same directory, they will be combined
# into the same sample.
# The pattern "myreads/{sample}.fq" will name this sample
# "HUMAN-Blank_S3_R1_001" and any R2 file in the same directory will
# be run separately as sample "HUMAN-Blank_S3_R2_001".
# ----------------------
# Advanced Regex Example
# ----------------------
# For files myreads/HUMAN-Blank-xx-xx-xxx-xxxx-xxx-xx_S1_R1_001.fastq and
# myreads/HUMAN-Sample1-xx-xx-xxx-xxxx-xxx-xx_S2_R1_001.fastq, where the
# x's represent values that are unique to each sample that you do not
# want included in the sample name you can use regex and/or a
# throwaway keyword.
# The pattern
# "myreads/{sample,\w+-\w+}-{remove}_R{pair}_001.fastq" would work
# for this example, resulting in sample names "HUMAN-Blank" and
# "HUMAN-Sample1".
# The throwaway keyword, "remove", is ignored and not included in the sample
# name. The regex following the comma in the "sample" wildcard helps
# constrain the name used for sample.

 
#fastq_pattern: /scratch/top23/aDNA/readsets/m_bison_readsets/{sample}_R{pair}_001.fastq
fastq_pattern: /scratch/top23/aDNA/readsets/sloth_readsets/5_final_readsets_2.0/{sample}.fq
#fastq_pattern: /scratch/top23/mtsv/bison/modern_bison/readsets/{sample}_{pair}.fastq
 


# QC is performed by fastp (https://github.com/OpenGene/fastp).
# Default parameters are used but these can be modified by passing
# parameters as a string. Run
# "fastp --help" for options (avoid changing input/output and
# thread parameters as these are handled in the snakemake rule.)
# To bypass QC filtering, use
# "--disable_adapter_trimming --disable_trim_poly_g
# --disable_quality_filtering --disable_length_filtering"

 
fastp_params: ""
 

# Reads will be chopped up into equal, non-overlapping
# KMER_SIZE queries for processing.

 
kmer_size: 36
 

# ===============================================================
#
# BINNING: Alignment-based binning of queries.
#
# ===============================================================

# Path to sequence database artifact json. This file is created
# during the database build step. Use `mtsv_setup --help` or
# see the "Sequence Download and Setup Quick Start Guide" in the
# wiki (https://github.com/FofanovLab/MTSv/wiki/) for help
# setting up the database.

 
database_config: /o-scratch/databases/MTSv_databases/Oct-28-2019/artifacts/genbank.json


# Edit distance to tolerate in matched reference sites.

 
edits: 3
 

# Set seed-based alignment parameters to a recommended mode
# for fast (more misses, fast runtime),
# efficient (med misses, med runtime),
# or sensitive (few misses, slow) runs.
# These modes set values for SEED_SIZE, MIN_SEEDS, and SEED_GAP to
# fast=17,5,2, efficient=14,4,2, sensitive=11,3,1.
# Adding values for seed_size, min_seeds, or seed_gap in this
# config file will override these settings.

 
binning_mode: efficient
 

# Inital exact match query size.
# Uncomment (remove "#" before name) to modify this parameter
# This will override the value set for the BINNING_MODE.

 
# seed_size: 
 

# Minimum number of seeds to perform alignment of a candidate site.
# Uncomment (remove "#" before name) to modify this parameter
# This will override the value set for the BINNING_MODE.

 
# min_seeds:
 

# Gap between seeds used for initial exact match.
# Uncomment (remove "#" before name) to modify this parameter
# This will override the value set for the BINNING_MODE.

 
# seed_gap: 

# ===============================================================
#
# SUMMARY: Ummm... I think...
#
# ===============================================================

max_taxa_per_query: 30

# ===============================================================
# 
# ANALYSIS: Perform statistical analysis on candidate taxa.
# The analysis compares the difference between expected and
# observed values for the ratio of unique signature hits to
# total unique hits.
#
# ===============================================================

# Provide a rule for filtering candidate taxa for analysis.
# "top" returns the top FILTER_VALUE taxa based on values
# in the FILTER_COLUMN column of the summary table.
# "min" returns taxa that have values higher than FILTER_VALUE
# in the FILTER_COLUMN column of the summary table.
# "in" returns only taxa in a list of taxa provided for FILTER_VALUE.
# Only species-level taxids should be provided.
# "quant" returns taxa in the FILTER_VALUE quantile for values in
# the FILTER_COLUMN column of the summary table.

 
filter_rule: min
 

# Provide the value to filter candidates.

 
filter_value: 200
 

# Define the summary data column to filter candidates.
# Choices: "unique_signature", "signature", "total",
# "unique", "weighted_support", "unique_weighted_support",
# "taxid"

 
filter_column: unique_signature
 

# To save processing time, expected values gathered during the analysis
# step will be saved and used in future runs (assuming that the same
# binning parameters and sequence databases are used).
# The same datastore path should be used for every run to avoid needing to
# recalculate values. The same datastore can be shared among
# multiple users if they have write access to the file.

 
datastore: /scratch/top23/databases/mtsv/expected_datastore.hdf5
 

# The expected values are calculated by simulating queries from
# the sequence database for each candidate species. SAMPLE_N_KMERS
# is the number of queries to sample.

 
sample_n_kmers: 100000
 

# Alpha cutoff for statistical analysis

 
alpha: 0.05
 

# The statistical analysis uses an equivalence test
# and the equivalence bounds are calculated based
# on Cohen's H effect size.


h: 0.5


# Seaborn's clustermap is used to generate heatmap figures.
# Keyword arguments (e.g. {'figsize': (10,10)}) can be passed to
# the function as a dictionary.


figure_kwargs: {}
 

# ===============================================================
#
# EXTRACT: Extract queries that hit a given taxid.
#
# ===============================================================
    
# Extract queries that hit taxids in this list.


extract_taxids: []

# ===============================================================
#
# WGFAST: SNP typing for strain-level resolution.
#
# ===============================================================

# Run wgfast on taxids in this list.


wgfast_taxids: []


# Reference directories that correspond to each taxid in WGFAST_TAXIDS.
# Use "wgfastdb" to setup the reference directories.


wgfast_reference_dirs: []


# Default WGFAST (https://github.com/jasonsahl/wgfast)
# parameters are used but parameters can be modified
# by passing them as a string. Run "wgfast --help" for
# options (avoid changing input/output and thread
# parameters as these are handled in the snakemake rule.)


wgfast_params: ""


# ===============================================================
#
# CONCOCT: Alignment-free binning of unaligned queries.
# See concoct documentation (https://github.com/BinPro/CONCOCT)
# for details on alignment free binning pipeline.
#
# ===============================================================

# The queries are assembled using Megahit
# (https://github.com/voutcn/megahit). Default parameters are
# used but parameters can be modified by passing them as a string.
# Run megahit --help for options (avoid changing input/output and thread
# parameters as these are handled in the snakemake rule.)


megahit_params: ""


# Concoct (https://github.com/BinPro/CONCOCT) cuts up contigs that
# are over a certain length. Default parameters are used but
# parameters can be modified by passing them as a string.
# Run "cut_up_fasta.py --help" for options (avoid changing
# input/output and thread parameters as these are handled
# in the snakemake rule.)


cutup_params: ""


# Queries are mapped to the contigs using BWA (https://github.com/lh3/bwa).
# Default parameters are used but parameters can be modified by passing
# them as a string. Run "bwa mem" for options (avoid changing input/output
# and thread parameters as these are handled in the snakemake rule.)


bwa_params: ""


# Concoct clusters contigs using nucleotide composition and coverage data
# across multiple samples (currently our pipeline runs samples separately).
# Default parameters are used but parameters can be modified by passing
# them as a string. Run "concoct --help" for options (avoid changing
# input/output and thread parameters as these are handled in the
# snakemake rule.) 


concoct_params: ""


